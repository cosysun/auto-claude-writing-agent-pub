DeepSeek又发论文了。

这次的主题有点意思：**他们发现，现在的大模型在浪费大量算力做一件很傻的事——用计算来模拟查字典**。

论文叫《Conditional Memory via Scalable Lookup》，核心是一个叫**Engram**的模块。

![图片](https://mmbiz.qpic.cn/mmbiz_png/HRdaeEmxNHZqVlAd5pVj652sibhX07KiawBVbiajNibsdY6yjEUrrdiabHdr2FNfWvLRdbCeg3g3F7vDJiaES8uicuYxw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

这个名字有点意思。Engram是神经科学术语，最早由德国生物学家Richard Semon在1904年提出，指的是大脑中存储记忆痕迹的物理结构——当你记住"巴黎是法国首都"这个事实时，这条信息就以某种物理形式（可能是特定的神经连接模式）存储在你的大脑里，这个物理痕迹就叫engram。

DeepSeek用这个名字，显然是想说：**我们要给大模型装上真正的"记忆"**。

说实话，看完之后我挺兴奋的——这篇论文的思路非常优雅，而且解决的是一个很根本的问题。更重要的是，它触及了一个认知科学的经典命题：**记忆和思考是什么关系？**

## 先说问题：大模型在浪费算力做"背书"

你有没有想过，当大模型看到"Diana, Princess of Wales"（戴安娜王妃）这个词的时候，它内部发生了什么？

DeepSeek在论文里引用了一个很有意思的研究（PatchScope）：模型需要**消耗多层Attention和FFN**，才能逐步把这个实体识别出来。

具体来说，模型处理"Wales"这个词时的内部状态演变：

|层数|模型内部理解|
|---|---|
|1-2层|"威尔士"（当成英国的一个地区）|
|3层|"欧洲的一个国家"|
|4层|"女性君主持有的头衔"（开始识别Princess）|
|5层|"威尔士王储的妻子"|
|6层|"戴安娜王妃（1961-1997），查尔斯王子的前妻"|

看到没？模型用了6层计算，才把一个固定的历史人物识别出来。

问题在于：**这个信息是静态的、固定的，根本不需要每次都"计算"出来**。

"亚历山大大帝"就是"亚历山大大帝"，"四大发明"就是"四大发明"，"张仲景"就是"张仲景"。这些固定搭配、命名实体、惯用表达，每次都用神经网络重新计算一遍，是不是有点傻？

这就像你每次需要查"中国首都是哪"的时候，不是直接查字典，而是从头推理一遍——中国是个国家，国家有首都，中国的政治中心在...

**DeepSeek的核心观点是：大模型浪费了大量的"网络深度"在做这种重复性的静态知识重建。这些算力本来可以用来做更有价值的事——比如推理。**

## Engram的核心思想：给模型发一本字典

想象你在考试。

以前的规则是：什么都不能带，全靠脑子现场推。"亚历山大大帝是谁？"你得从头想——亚历山大，希腊名字，大帝说明是君主，历史上有名的希腊君主...

现在新规则：**允许带一本字典进考场**。字典里写着"亚历山大大帝 = 马其顿国王，公元前356-323年，征服了波斯帝国"。你直接翻到这一页，抄上去，省下来的时间做后面的推理题。

Engram就是这本字典。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

具体怎么查？很简单：

模型看到"Alexander the Great"这三个词连在一起，就像看到字典的索引词条。它用一个很快的方法（哈希）定位到字典里对应的那一页，直接把预先存好的信息拿出来用。

**整个过程不需要"思考"，只需要"翻页"。**

但这里有个问题：同一个词在不同场合意思不一样。

比如"苹果"，可能是水果，也可能是那家科技公司。字典里存的是哪个意思？

Engram的解决方案很聪明：**查完字典之后，先看看上下文，再决定用不用**。

如果前面在聊水果，字典里查出来的"苹果公司"就不太对劲，模型会自动忽略这个查表结果，继续用自己的推理。如果前面在聊手机，那字典里的信息就很有用，直接采纳。

这就像一个聪明的学生：带了字典进考场，但不是无脑抄，而是先判断字典里的答案和题目对不对得上。

## 关键发现：U型缩放定律

这里是论文最有意思的部分。

DeepSeek研究了一个问题：**如果总参数量固定，应该把多少参数分配给MoE专家，多少分配给Engram记忆？**

他们定义了一个"分配比例"ρ：

-   ρ = 100% 表示纯MoE（所有稀疏参数都给专家）
    
-   ρ < 100% 表示把部分参数从专家转移到Engram
    

实验结果让人惊讶：

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

**验证损失呈现U型分布**：

-   纯MoE（ρ=100%）不是最优的
    
-   分配约20-25%给Engram（ρ≈75-80%）效果最好
    
-   把太多参数给Engram（ρ<50%）效果又变差
    

这个U型曲线说明了什么？

**MoE和Engram是互补的**：

-   MoE擅长动态的、需要上下文推理的任务
    
-   Engram擅长静态的、固定模式的识别
    

两者缺一不可。纯MoE缺少记忆能力，纯Engram缺少推理能力。

## 插一段：博尔赫斯早就写过这个

看到这个U型曲线的时候，我突然想起博尔赫斯的一个短篇：\*\*《博闻强记的富内斯》\*\*（Funes the Memorious）。

故事讲的是一个叫富内斯的阿根廷青年，从马上摔下来之后，获得了"完美记忆"的能力——他能记住一切。每一片叶子的形状，每一朵云的变化，甚至能记住1882年4月30日黎明时分南方天空的云彩排列。

但博尔赫斯写道：**富内斯无法思考**。

> "思考就是忘记差异，就是概括，就是抽象。在富内斯塞满了东西的世界里，只有细节，几乎是直接感知的细节。"

富内斯能记住三个不同时刻看到的同一条狗，但他无法理解"狗"这个概念——因为每一条狗、每一个瞬间的狗，对他来说都是完全不同的东西。他记住了一切，却失去了抽象的能力。

**这不就是论文里U型曲线的左端吗？**

当ρ趋近于0（全是Engram，没有MoE）时，模型有无限的记忆，但失去了推理能力。它能记住"亚历山大大帝"是谁，但无法用这些知识进行推理。

反过来，当ρ=100%（全是MoE，没有Engram）时，模型有强大的推理能力，但要浪费大量算力重建那些本可以直接记住的东西。

博尔赫斯在1942年就洞察到了这一点：**记忆和思考是互补的，但也是对立的**。完美的记忆会杀死思考，而纯粹的思考则需要不断重新发明轮子。

最优解在中间——既有记忆，又有思考。

DeepSeek的实验数据给出了一个惊人精确的答案：**大约75-80%给思考，20-25%给记忆**。

这让我想到另一个认知心理学的经典概念：\*\*组块（Chunking）\*\*。

1956年，心理学家George Miller发表了著名的论文《神奇的数字7±2》，指出人类工作记忆的容量是有限的，但我们可以通过"组块"来扩展它。比如记电话号码138-8888-6666，你不是记11个数字，而是记3个组块。

N-gram本质上就是语言的组块。"亚历山大大帝"不是5个字，而是1个组块。Engram做的事情，就是把这些组块预先存好，省得每次都要重新计算。

**人脑早就在这么干了**。DeepSeek只是让大模型学会了同样的技巧。

## 实验结果：推理能力提升比知识提升更大

这是让我最惊讶的部分。

你可能会想：Engram是个"记忆模块"，应该主要提升知识类任务吧？

确实，知识任务有提升：

-   MMLU：+3.4
    
-   CMMLU：+4.0
    
-   MMLU-Pro：+1.8
    

但**推理任务的提升更大**：

-   BBH：+5.0
    
-   ARC-Challenge：+3.7
    
-   DROP：+3.3
    

甚至代码和数学也有显著提升：

-   HumanEval：+3.0
    
-   MATH：+2.4
    
-   GSM8K：+2.2
    

等等，一个"记忆模块"为什么能提升推理能力？

## 机制分析：为什么"记忆模块"能提升推理？

这是我最想搞明白的问题。

DeepSeek做了一个很有意思的实验：他们"偷看"模型每一层在想什么。

具体方法是：把每一层的中间结果拿出来，问它"你现在觉得下一个词是什么？"。如果这一层已经很接近最终答案，说明模型在这一层就基本"想明白了"。

结果很直观：

**有Engram的模型，在很早的层就"想明白了"；没有Engram的模型，要到很深的层才行。**

为什么？

因为没有字典的模型，前面几层都在忙着做一件事：**搞清楚"亚历山大大帝"是谁**。它得一层一层地拼凑——这是个人名，是个历史人物，是个国王，是马其顿的国王...

等它终于搞清楚这是谁了，已经用掉了5、6层。剩下的层才能开始真正的推理。

但有字典的模型不一样。第2层的时候，Engram直接告诉它："亚历山大大帝 = 马其顿国王，征服者"。好了，搞定，后面20多层全部用来推理。

**这就像两个学生做同一张卷子**：

一个学生得先花20分钟背公式，再用40分钟做题。

另一个学生带了公式表，60分钟全用来做题。

谁的推理题做得更好？显然是第二个。

DeepSeek还做了一个更精确的测量：**Engram模型第5层的"思考深度"，相当于普通模型第12层的水平**。

换句话说，Engram相当于免费给模型加了7层深度。

这就解释了为什么推理能力提升这么大——不是Engram本身能推理，而是它**把推理的空间让出来了**。

## 长上下文能力也炸了

还有个意外收获：处理长文章的能力暴涨。

有个测试叫"大海捞针"——在一篇很长的文章里藏一句关键信息，看模型能不能找到。

|任务|没有字典|有字典|
|---|---|---|
|多问题大海捞针|84.2%|97.0%|
|变量追踪|77.0%|89.0%|

为什么字典能帮助处理长文章？

想象你在读一本很长的小说。如果你每次看到"福尔摩斯"都要停下来想"这是谁来着..."，读到后面肯定记不住前面的剧情。

但如果"福尔摩斯 = 侦探，住贝克街221B"这个信息已经存在字典里，你的注意力就可以全部用来追踪剧情——谁杀了谁，线索在哪，凶手是谁。

**Engram处理了"这是谁"的问题，Attention就可以专注于"发生了什么"的问题。**

相当于给大脑减负了。

## 系统设计：字典可以放在抽屉里

这里体现了DeepSeek一贯的风格：**理论创新和工程落地并重**。

继续用考试的比喻。

MoE（专家模型）的问题是：每道题都要"现场"决定找哪个专家来答，这个决定本身就要花时间。

但字典不一样。你看到"亚历山大大帝"，就知道要翻到A开头那一页。**你不需要先读完整道题，才知道去查哪个词条**。

这意味着什么？

意味着字典可以**提前准备好**。

模型还在处理第1层的时候，系统就已经知道第2层要查什么词条了。所以可以提前把那一页准备好，等模型算到第2层的时候，字典已经翻开摆在那儿了。

更妙的是：**字典不需要放在桌上，放在抽屉里也行**。

GPU显存很贵，就像桌面空间有限。但CPU内存便宜得多，就像抽屉容量大得多。

既然可以提前知道要查什么，那就提前从抽屉里把那一页拿出来，等用的时候已经在桌上了。

DeepSeek做了个实验：把一本1000亿参数的"字典"放在抽屉里（CPU内存），结果：

|配置|速度|
|---|---|
|不带字典|9,031 字/秒|
|带1000亿参数字典（放抽屉里）|8,858 字/秒|

**只慢了2%** ，但多了1000亿参数的知识。

这就是为什么Engram可以做得很大——字典放抽屉里就行，不占桌面。

## 门控可视化：确实在识别固定模式

论文最后有个很直观的可视化：

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

红色表示门控值高（Engram被激活），白色表示门控值低（Engram被忽略）。

可以看到，门控在这些地方激活：

-   "Alexander the Great"（亚历山大大帝）
    
-   "the Milky Way"（银河系）
    
-   "Princess of Wales"（威尔士王妃）
    
-   "四大发明"
    
-   "张仲景"
    
-   "伤寒杂病论"
    

全是命名实体和固定搭配。Engram确实在做它该做的事：识别静态模式。

## 往大了说：DeepSeek在开一条新路

回到开头的问题：这篇论文的意义是什么？

过去几年，大家都在一个方向上卷：**怎么让模型算得更聪明**。MoE让不同的专家处理不同的问题，Attention让模型看到更远的上下文，更深的网络让推理更复杂。

但不管怎么卷，本质上都是在优化"计算"。

DeepSeek说：等等，**有些问题根本不需要算，查一下就行了**。

这个思路其实很符合直觉：人脑也不是什么都靠推理，很多时候就是直接调用记忆。你看到"1+1"不需要推理，直接输出"2"就行。

论文最后一句话很有意思：

> "We envision conditional memory as an indispensable modeling primitive for next-generation sparse models."

翻译过来：我们认为条件记忆会成为下一代稀疏模型的基础组件。

DeepSeek在押注一个新的架构方向。

## 最后：记忆与思考的平衡

回到开头的问题：记忆和思考是什么关系？

博尔赫斯用富内斯告诉我们：**完美的记忆会杀死思考**。认知心理学告诉我们：人脑用组块来平衡记忆和思考的负担。

现在DeepSeek用实验数据告诉我们：**最优的比例大约是75%计算 + 25%记忆**。

这个数字让我觉得很有意思。它意味着，即使是"智能"系统，也不能全靠"聪明"——你得记住一些东西，才能把脑力用在更值得思考的地方。

这篇论文给我最大的启发是：**有时候最好的优化不是让计算更快，而是把计算变成查表**。

O(1)的查表永远比O(n)的计算快。如果一个问题的答案是固定的、可以预先算好存起来的，那就没必要每次都重新算。

这个道理在计算机科学里叫"空间换时间"。但在大模型领域，过去几年大家都在卷MoE、卷Attention、卷更深的网络，似乎忘了还有"记忆"这条路。

DeepSeek的Engram提醒我们：**大模型不是越大越好、也不是越深越好，关键是把合适的任务分配给合适的模块**。

静态知识 → 查表（Engram）

动态推理 → 计算（MoE）

就像人脑一样：你不需要每次看到"1+1"都重新推导，直接从记忆里调出"2"就行了。省下来的脑力，用来思考更有价值的问题。

富内斯记住了一切，却无法思考。

纯MoE模型能够思考，却要浪费算力重建记忆。

**最聪明的系统，是知道什么该记住、什么该思考的系统。**

___

**参考资料**：

-   Engram论文：https://github.com/deepseek-ai/Engram
    
-   DeepSeek-V3技术报告：https://arxiv.org/abs/2412.19437
    
-   mHC论文：https://arxiv.org/abs/2512.24880